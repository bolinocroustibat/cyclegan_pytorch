{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a044cebd-be9b-4be9-8583-ed83c2415724",
   "metadata": {},
   "source": [
    "# CycleGAN in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e80a3-34ef-4b5e-9d9a-68448ddce7ba",
   "metadata": {},
   "source": [
    "This youtube video from \"Two minutes paper\" provides a good summary of the method we are going to implement in this notebook:  \n",
    "\n",
    "[![AI Learns to Synthesize Pictures of Animals | Two Minute Papers #152](https://img.youtube.com/vi/D4C1dB9UheQ/0.jpg)](https://www.youtube.com/embed/D4C1dB9UheQ)\n",
    "\n",
    "I also recommend you to take a look at the project page by its authors: https://junyanz.github.io/CycleGAN/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab6a7f-944a-4360-b8dd-57b226e0f4fb",
   "metadata": {},
   "source": [
    "## Hyperparameters (on top for convinience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe619df7-7113-4061-89af-f0be7e5cacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0 # epoch to start training from\n",
    "n_epochs = 5 # number of epochs of training\n",
    "dataset_name = \"horse2zebra\" # name of the dataset\n",
    "batch_size = 1 # size of the batches\n",
    "lr = 0.0002 # adam: learning rate\n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "decay_epoch = 1 # epoch from which to start lr decay\n",
    "n_cpu = 10 # number of cpu threads to use during batch generation\n",
    "img_height = 256 # size of image height\n",
    "img_width = 256 # size of image width\n",
    "channels = 3 # number of image channels\n",
    "sample_interval = 100 # interval between saving generator outputs\n",
    "checkpoint_interval = -1 # interval between saving model checkpoints\n",
    "n_residual_blocks = 9 # number of residual blocks in generator\n",
    "lambda_cyc = 10.0 # cycle loss weight\n",
    "lambda_id = 5.0 # identity loss weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c9672-6401-4753-afbc-480d4066a83b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af2f54-5b13-4a57-9774-1c5858a96361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid, save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a633ba8a-c30b-42d1-a10c-27f4c0fa605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will run on CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"This notebook will run on GPU.\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"This notebook will run on CPU.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595da249-a107-4795-a89d-875925bf256d",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6813cc49-3829-45da-851d-5b789dfb7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions to download the dataset\n",
    "# this code comes mainly from gluoncv.utils\n",
    "def download(url, path=None, overwrite=False) -> str:\n",
    "    \"\"\"Download an given URL.\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL to download\n",
    "    path : str, optional\n",
    "        Destination path to store downloaded file. By default stores to the\n",
    "        current directory with same name as in url.\n",
    "    overwrite : bool, optional\n",
    "        Whether to overwrite destination file if already exists.\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The file path of the downloaded file.\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        fname = url.split(\"/\")[-1]\n",
    "    else:\n",
    "        path = os.path.expanduser(path)\n",
    "        if os.path.isdir(path):\n",
    "            fname = os.path.join(path, url.split(\"/\")[-1])\n",
    "        else:\n",
    "            fname = path\n",
    "\n",
    "    if overwrite or not os.path.exists(fname):\n",
    "        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        print(\"Downloading %s from %s...\" % (fname, url))\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code != 200:\n",
    "            raise RuntimeError(\"Failed downloading url %s\" % url)\n",
    "        total_length = r.headers.get(\"content-length\")\n",
    "        with open(fname, \"wb\") as f:\n",
    "            if total_length is None:  # no content length header\n",
    "                for chunk in r.iter_content(chunk_size=1024):\n",
    "                    if chunk:  # filter out keep-alive new chunks\n",
    "                        f.write(chunk)\n",
    "            else:\n",
    "                total_length = int(total_length)\n",
    "                for chunk in tqdm(\n",
    "                    r.iter_content(chunk_size=1024),\n",
    "                    total=int(total_length / 1024.0 + 0.5),\n",
    "                    unit=\"KB\",\n",
    "                    unit_scale=False,\n",
    "                    dynamic_ncols=True,\n",
    "                ):\n",
    "                    f.write(chunk)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def download_dataset(\n",
    "    dataset_name: str, data_path: str = \"data/\", overwrite: bool = False\n",
    ") -> None:\n",
    "    compatible_datasets = [\n",
    "        \"ae_photos\",\n",
    "        \"apple2orange\",\n",
    "        \"cezanne2photo\",\n",
    "        \"cityscapes\",\n",
    "        \"facades\",\n",
    "        \"grumpifycat\",\n",
    "        \"horse2zebra\",\n",
    "        \"iphone2dslr_flower\",\n",
    "        \"maps\",\n",
    "        \"mini\",\n",
    "        \"mini_colorization\",\n",
    "        \"mini_colorization\",\n",
    "        \"mini_pix2pix\",\n",
    "        \"monet2photo\",\n",
    "        \"summer2winter_yosemi\",\n",
    "        \"ukiyoe2photo\",\n",
    "        \"vangogh2photo\",\n",
    "    ]\n",
    "    if dataset_name not in compatible_datasets:\n",
    "        print(\"The dataset you chose is not compatible.\")\n",
    "        print(f\"Please select one among: {compatible_datasets}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    download_url = f\"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{dataset_name}.zip\"\n",
    "    download_dir = os.path.join(data_path, \"downloads\")\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.mkdir(download_dir)\n",
    "\n",
    "    filename = download(download_url, path=download_dir, overwrite=overwrite)\n",
    "\n",
    "    # Extract archive in target dir\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(path=data_path)\n",
    "\n",
    "    # Re-organize dirs for more clarity\n",
    "    testdir = data_path + dataset_name + \"/\" + \"test\"\n",
    "    traindir = data_path + dataset_name + \"/\" + \"train\"\n",
    "    if not os.path.exists(testdir):\n",
    "        os.mkdir(testdir)\n",
    "    if not os.path.exists(traindir):\n",
    "        os.mkdir(traindir)\n",
    "\n",
    "    os.rename(data_path + dataset_name + \"/\" + \"trainA\", traindir + \"/A\")\n",
    "    os.rename(data_path + dataset_name + \"/\" + \"trainB\", traindir + \"/B\")\n",
    "    os.rename(data_path + dataset_name + \"/\" + \"testA\", testdir + \"/A\")\n",
    "    os.rename(data_path + dataset_name + \"/\" + \"testB\", testdir + \"/B\")\n",
    "\n",
    "    # Done\n",
    "    print(f\"Dataset downloaded and extracted in '{data_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f8b8c1-673a-486c-93ba-a5395a843cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset(\"monet2photo\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9a177-60ec-466c-a4a6-2afa0b1e41c1",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224a68aa-a356-42ed-8195-aafb28adbbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesData(Dataset):\n",
    "    def __init__(self, root, data_augmentations=None, unaligned=False, dataset=\"train\"):\n",
    "        self.data_augmentations = data_augmentations\n",
    "        self.unaligned = unaligned\n",
    "        \n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, f\"{dataset}/A\") + \"/*.*\"))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, f\"{dataset}/B\") + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        #if image_A.mode != \"RGB\":\n",
    "        #    image_A = to_rgb(image_A)\n",
    "        #if image_B.mode != \"RGB\":\n",
    "        #    image_B = to_rgb(image_B)\n",
    "\n",
    "        item_A = self.data_augmentations(image_A)\n",
    "        item_B = self.data_augmentations(image_B)\n",
    "        return {\"A\": item_A, \"B\": item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_rgb(image):\n",
    "        rgb_image = Image.new(\"RGB\", image.size)\n",
    "        rgb_image.paste(image)\n",
    "        return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9309c6-72a7-4aa3-9f08-7cda9ba2e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization values are the ones from ImageNet\n",
    "# mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "\n",
    "# Training Data augmentations\n",
    "train_data_augmentations = transforms.Compose([\n",
    "    transforms.Resize(int(img_height * 1.12), transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomCrop((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Test Data augmentations\n",
    "test_data_augmentations = transforms.Compose([\n",
    "    transforms.Resize(int(img_height), transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7f19e0-7538-4c04-b916-3076ef147c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Loader\n",
    "train_loader = DataLoader(\n",
    "    ImagesData(f\"data/{dataset_name}\", data_augmentations=train_data_augmentations, unaligned=True),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# Test Data Loader\n",
    "test_loader = DataLoader(\n",
    "    ImagesData(f\"data/{dataset_name}\", data_augmentations=test_data_augmentations, unaligned=True, dataset=\"test\"),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30ae88-96af-476f-87fa-898b21c28d2c",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7de85-365c-4ebb-8223-b9c8069ba722",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "From the paper, Appendix, 7.2. Network architectures: \n",
    "\n",
    "\"We  adopt  our  architectures from  Johnson  et  al. [23].  \n",
    "We  use 6 residual  blocks  for 128×128 training images,  \n",
    "and 9 residual blocks for 256×256 or higher-resolution  \n",
    "training images. Below, we followthe naming convention  \n",
    "used in the Johnson et al.’s Github repository.  \n",
    "Let c7s1-k denote a 7×7 Convolution-InstanceNorm-ReLU layer  \n",
    "with k filters and stride 1. dk denotes a 3×3 Convolution-InstanceNorm-ReLU   \n",
    "layer with k filters and stride 2. Reflection padding was  \n",
    "used to reduce artifacts. Rk denotes a residual block that  \n",
    "contains two 3×3 convolutional layers with the same number  \n",
    "of filters on both layer. uk denotes a 3×3 fractional-strided-Convolution-InstanceNorm-ReLU  \n",
    "layer with k filters and stride 1/2. \n",
    "\n",
    "The network with 6 residual blocks consists of:  \n",
    "c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,u128,u64,c7s1-3  \n",
    "The network with 9 residual blocks consists of:  \n",
    "c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,R256,R256,R256,u128u64,c7s1-3\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b6b1cfa-fa66-4300-bc6b-1e0244f5527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement first the residual block.\n",
    "# We will re-use it many times\n",
    "# in the code.\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d63cbec1-2a64-4e70-b294-5110aa2ed234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's implement the generator according\n",
    "# to the paper.\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape: list, num_residual_blocks: int = 9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        # c7s1-64\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        # d128, d256\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        # R256 * <num_residual_blocks> (6 or 9 in the paper)\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        # u128, # u64\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        # c7s1-3\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(out_features, channels, 7),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b0a5b-f10a-4f35-8ae0-10b8980425bd",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "From the paper, Appendix, 7.2. Network architectures:\n",
    "\n",
    "For discriminator networks, we use 70×70 PatchGAN [22].  \n",
    "Let Ck denote a 4×4 Convolution-InstanceNorm-LeakyReLU  \n",
    "layer with k filters and stride 2. After the last layer,  \n",
    "we apply a convolution to produce a1-dimensional output.  \n",
    "We do not use InstanceNorm for the first C64 layer.  \n",
    "We use leaky ReLUs with a slope of 0.2.  \n",
    "\n",
    "The discriminator architecture is:  \n",
    "C64-C128-C256-C512  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "918da47f-7e7e-4246-b61b-0379c8dc93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape: list):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            # C64\n",
    "            *discriminator_block(64, 128),\n",
    "            # C128\n",
    "            *discriminator_block(128, 256),\n",
    "            # C256\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            # C512\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5ded5d-d235-42fe-a57c-46b6157ba477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample and checkpoint directories\n",
    "os.makedirs(f\"images/{dataset_name}\", exist_ok=True)\n",
    "os.makedirs(\"saved_models/{dataset_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59cbf66-5840-4c1d-9734-85c21f26265a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1Loss()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "criterion_GAN.to(device)\n",
    "criterion_cycle.to(device)\n",
    "criterion_identity.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2f84c-aa97-42bd-b0c3-fb73f1343553",
   "metadata": {},
   "source": [
    "## Initializing our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "724514f7-9478-454a-a03f-ff501351c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95460197-1c2a-49be-949c-c39efc14d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "G_AB = Generator(input_shape, n_residual_blocks)\n",
    "G_BA = Generator(input_shape, n_residual_blocks)\n",
    "D_A = Discriminator(input_shape)\n",
    "D_B = Discriminator(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5ae057f-a56a-4154-9811-664db4b3ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7934405f-2653-4a7e-a678-8fba47a3dead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): ZeroPad2d(padding=(1, 0, 1, 0), value=0.0)\n",
       "    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "G_AB.apply(weights_init_normal)\n",
    "G_BA.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05fb9c47-eade-4aed-aac6-e35b7152ce29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): ZeroPad2d(padding=(1, 0, 1, 0), value=0.0)\n",
       "    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_AB.to(device)\n",
    "G_BA.to(device)\n",
    "D_A.to(device)\n",
    "D_B.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78a2b8-9053-46c1-9e37-a3749580dcd5",
   "metadata": {},
   "source": [
    "G_AB.to(device)\n",
    "G_BA.to(device)\n",
    "D_A.to(device)\n",
    "D_B.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821d056-d64c-4fa7-97ba-8961aba8858e",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fec6e9a3-2511-4bca-b6f3-4b9e01749b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0866b7-3990-42cb-b552-d606b631b768",
   "metadata": {},
   "source": [
    "## Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e23a9e66-25fb-4e72-b237-055bc7b46017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6596dd5d-f3ce-4188-853e-9219a4998b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bae93407-c962-4629-9e77-892ba3382062",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "else:\n",
    "    Tensor = torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d0f2d-9bd0-4c24-8805-9a7bbc1d41d8",
   "metadata": {},
   "source": [
    "From the paper, 4. Implemention, Training details: \n",
    "\n",
    "[...] to  reduce  model  oscillation  [15],   \n",
    "we  follow Shrivastava  et  al.’s  strategy  [46]   \n",
    "and  update  the  discriminators using a history  \n",
    "of generated images rather than the ones produced  \n",
    "by the latest generators.  We keep an image buffer  \n",
    "that stores the 50 previously created images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47f3f0a3-a2df-4d83-acec-0f2acf0af1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bd7cb86-1993-44a7-bf4c-b5bc0256634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ImageBuffer()\n",
    "fake_B_buffer = ImageBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fc61f21-fde6-4240-8b49-a51b4f0a7339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(test_loader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    save_image(image_grid, f\"images/{dataset_name}/{batches_done}.png\", normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c045c-2968-4a11-a4fe-e6f7446bd69b",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d118e1a-b39f-41cf-911e-b01697ab97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "    \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "        expects epoch to start from 1.\n",
    "    \"\"\"\n",
    "    x = range(1, epoch+1)\n",
    "    y = np.concatenate((train_loss, valid_loss))\n",
    "    graphs = [[x,train_loss], [x,valid_loss]]\n",
    "    x_margin = 0.2\n",
    "    y_margin = 0.05\n",
    "    x_bounds = [1-x_margin, epochs+x_margin]\n",
    "    y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2536a05a-1d98-4226-a602-b00e52be6891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bfa988e8339a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mloss_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projets/cyclegan-pytorch/.venv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projets/cyclegan-pytorch/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "mb = master_bar(range(epoch, n_epochs))\n",
    "prev_time = time.time()\n",
    "history_loss_D = []\n",
    "history_loss_G = []\n",
    "for epoch in mb:\n",
    "#for epoch in range(epoch, n_epochs):\n",
    "    for i, batch in enumerate(progress_bar(train_loader, parent=mb)):\n",
    "    #for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        # Set model input\n",
    "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
    "        \n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "        \n",
    "        G_AB.train()\n",
    "        G_BA.train()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            with autocast():\n",
    "                # Identity loss\n",
    "                loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "                loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "        else:\n",
    "            # Identity loss\n",
    "            loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "            loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            with autocast():\n",
    "                # GAN loss\n",
    "                fake_B = G_AB(real_A)\n",
    "                loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "                fake_A = G_BA(real_B)\n",
    "                loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "        else:\n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "            fake_A = G_BA(real_B)\n",
    "            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            with autocast():\n",
    "                # Cycle loss\n",
    "                recov_A = G_BA(fake_B)\n",
    "                loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "                recov_B = G_AB(fake_A)\n",
    "                loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "        else:\n",
    "            # Cycle loss\n",
    "            recov_A = G_BA(fake_B)\n",
    "            loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "            recov_B = G_AB(fake_A)\n",
    "            loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            scaler.scale(loss_G).backward()\n",
    "            scaler.step(optimizer_G)\n",
    "        \n",
    "        else:\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator A\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            with autocast():\n",
    "                # Real loss\n",
    "                loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "                # Fake loss (on batch of previously generated samples)\n",
    "                fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "                loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "        else:\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "            loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "            \n",
    "        # Total loss\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            scaler.scale(loss_D_A).backward()\n",
    "            scaler.step(optimizer_D_A)\n",
    "        else:\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator B\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            with autocast():\n",
    "                # Real loss\n",
    "                loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "                # Fake loss (on batch of previously generated samples)\n",
    "                fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "                loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "        else:\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "            loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "            \n",
    "        # Total loss\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            scaler.scale(loss_D_B).backward()\n",
    "            scaler.step(optimizer_D_B)\n",
    "        else:\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            scaler.update()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "        history_loss_D.append(loss_D.item())\n",
    "        history_loss_G.append(loss_G.item())\n",
    "        plot_loss_update(epoch, n_epochs, mb, history_loss_D, history_loss_G)\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(train_loader) + i\n",
    "        #batches_left = n_epochs * len(train_loader) - batches_done\n",
    "        #time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        #prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        #sys.stdout.write(f\"\\r[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(train_loader)}] [D loss: {loss_D.item()}] [G loss: {loss_G.item()}, adv: {loss_GAN.item()}, cycle: {loss_cycle.item()}, identity: {loss_identity.item()}] ETA: {time_left}\")\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_images(batches_done)\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(G_AB.state_dict(), f\"saved_models/{dataset_name}/G_AB_{epoch}.pth\")\n",
    "        torch.save(G_BA.state_dict(), f\"saved_models/{dataset_name}/G_BA_{epoch}.pth\")\n",
    "        torch.save(D_A.state_dict(), f\"saved_models/{dataset_name}/D_A_{epoch}.pth\")\n",
    "        torch.save(D_B.state_dict(), f\"saved_models/{dataset_name}/D_B_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd87e6-7cc4-4e67-b97c-e4c71c56d86e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
